{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Welcome To RP-Helpdesk Docs","text":"<p>Here you can find the documentation that we have used to make our servers and services run.</p> <p></p> <p>These guides are made for our own infrastructur, so they may not work for you.</p> <p></p> <p>Learn more about the software and OS's we use.</p>"},{"location":"about.html","title":"About RP-Helpdesk Docs","text":"<p>About RP-Helpdesk Docs</p> <p>This will be used as documentation and guides.</p> <p>You may have to change values, passwords and usernames etc.</p> <p>We are running Proxmox as the \"base\" OS on our servers, only one server is running another OS (Unraid) and this server is used by the Proxmox Cluster for shared storage to enable high availability. </p>"},{"location":"ansible/ansible-playbook.html","title":"Configure a Ansible Playbook","text":""},{"location":"ansible/ansible-playbook.html#windows-test-environment","title":"Windows Test Environment","text":"<p>Our Pre-made Playbooks</p> Configure templates Understand the variables Set your variable What your variables a used for Fully made Playbook for Windows Test Environment"},{"location":"ansible/ansible-playbook.html#configure-templates","title":"Configure templates","text":"<p>Setup the VMs that you want the playbook to deploy for you. This will have to be a full configuration of the functionality that you want from the VM. The playbook will not do anything to the VMs themselves, it will just clone the template of the configured VM.</p> Machine Name User Windows Server Main RPT-Main Administrator Windows Server Backup RPT-Back Administrator Windows Client 1 Client-1 Bruger Windows Client 2 Client-2 Bruger Windows Client 3 Client-3 Bruger Windows Server Main Windows Server Backup AD-DC AD-DC DNS DNS Fully Updated Fully Updated"},{"location":"ansible/ansible-playbook.html#understand-the-variables","title":"Understand the variables","text":"Variables Functions Win_Srv1 The name of the new server VM Win_10_1 The name of the new client VM apihost Proxmox Server IP apiuser Proxmox Username, password and method (PAM or PVE) apipass Proxmox Password prox_storage Storage location on Proxmox Server prox-node The Proxmox Node to Deploy the VM to disk_format The format for the VM disk timeout Wait for the Proxmox server to clone the VM pause Give the Proxmox server some time to recover from each cloning state The State The VM Will Be Put Into After Cloning (Can also be present if you don't want more then one, can also be skipped if none is needed)"},{"location":"ansible/ansible-playbook.html#set-your-variables","title":"Set Your variables","text":"<pre><code>- set_fact:\n        Win_Srv1: \"Windows-ADDC-1\" # The target 1 VM name\n        Win_Srv2: \"Windows-ADDC-2\" # The target 2 VM name\n        Win_10_1: \"Windows-Client-1\" # The target 3 VM name\n        Win_10_2: \"Windows-Client-2\" # The target 4 VM name\n        Win_10_3: \"Windows-Client-3\" # The target 5 VM name\n        CloneWin_Srv1: \"windowsservertemplate\" # The 1 VM source\n        CloneWin_Srv2: \"windowsservertemplate2\" # The 2 VM source\n        CloneWin_10_1: \"windowstemplate\" # The 3 VM source\n        CloneWin_10_2: \"windowstemplate2\" # The 4 VM source\n        CloneWin_10_3: \"windowstemplate3\" # The 5 VM source\n        apipass: \"*\" # The Password Used For Signing In To Proxmox\n        apihost: \"100.124.109.92\" # The Proxmox Host IP Address\n        apiuser: \"root@pam\" # The Proxmox User With Access\n        prox_storage: \"vms-ssd\" # The Target Storage On Proxmox\n        prox_node: \"main\" # The Target Node In The Proxmox Cluster\n        disk_format: \"qcow2\" # The Format For The Disk .img\n        timeout: \"500\" # The Task Can Take a While. Adapt\n        pause: \"10\" # How long The Pause Between Tasks Is (Recommend to adjust high if your server is not so strong)\n        state: \"started\" # The State The VM Will Be Put Into After Cloning (Can also be present if you don't want more then one, can also be skipped if none is needed)\n</code></pre>"},{"location":"ansible/ansible-playbook.html#what-your-variables-a-used-for","title":"What your variables a used for","text":"<p>The first section is for cloning the VM.</p> <p>The second section is for start the VM after it is done cloning. <pre><code>- proxmox_kvm:\n    api_user    : \"{{ apiuser }}\"\n    api_password: \"{{ apipass }}\"\n    api_host    : \"{{ apihost }}\"\n    clone       : \"{{ CloneWin_Srv1 }}\"   \n    name        : \"{{ Win_Srv1 }}\"  \n    node        : \"{{ prox_node }}\"\n    storage     : \"{{ prox_storage }}\"\n    format      : \"{{ disk_format }}\"\n    timeout     : \"{{ timeout }}\"  \n- name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win_Srv1 }}\"\n  ansible.builtin.pause:\n    seconds: \"{{ pause }}\"\n\n---------------------------------\n\n- name: start \"{{ Win_Srv1 }}\"\n  proxmox_kvm:\n    api_user: \"{{ apiuser }}\"\n    api_password: \"{{ apipass }}\"\n    api_host: \"{{ apihost }}\"\n    name: \"{{ Win_Srv1 }}\"\n    node: \"{{ prox_node }}\"\n    state: \"{{ state }}\"\n- name: Pause for \"{{ pause }}\" seconds to start the \"{{ Win_Srv1 }}\"\n  ansible.builtin.pause:\n    seconds: \"{{ pause }}\"\n</code></pre></p>"},{"location":"ansible/ansible-playbook.html#fully-made-playbook-for-windows-test-environment","title":"Fully made Playbook for Windows Test Environment","text":"<p>Please note:</p> <p>That all of our playbooks has pre-defined \"hosts\" in this case it is proxmox.</p> <p>This is because in our ansible hosts file our Proxmox server is named proxmox. <pre><code>- name: Test Environment\n  hosts: proxmox\n  tasks:\n   - set_fact:\n        Win_Srv1: \"Windows-ADDC-1\" # The target 1 VM name\n        Win_Srv2: \"Windows-ADDC-2\" # The target 2 VM name\n        Win_10_1: \"Windows-Client-1\" # The target 3 VM name\n        Win_10_2: \"Windows-Client-2\" # The target 4 VM name\n        Win_10_3: \"Windows-Client-3\" # The target 5 VM name\n        CloneWin_Srv1: \"windowsservertemplate\" # The 1 VM source\n        CloneWin_Srv2: \"windowsservertemplate2\" # The 2 VM source\n        CloneWin_10_1: \"windowstemplate\" # The 3 VM source\n        CloneWin_10_2: \"windowstemplate2\" # The 4 VM source\n        CloneWin_10_3: \"windowstemplate3\" # The 5 VM source\n        apipass: \"*\" # The Password Used For Signing In To Proxmox\n        apihost: \"100.124.109.92\" # The Proxmox Host IP Address\n        apiuser: \"root@pam\" # The Proxmox User With Access\n        prox_storage: \"vms-ssd\" # The Target Storage On Proxmox\n        prox_node: \"main\" # The Target Node In The Proxmox Cluster\n        disk_format: \"qcow2\" # The Format For The Disk .img\n        timeout: \"500\" # The Task Can Take a While. Adapt\n        pause: \"10\" # How long The Pause Between Tasks Is (Recommend to adjust high if your server is not so strong)\n        state: \"started\" # The State The VM Will Be Put Into After Cloning (Can also be present if you don't want more then one, can also be skipped if none is needed)\n    - proxmox_kvm:\n        api_user    : \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host    : \"{{ apihost }}\"\n        clone       : \"{{ CloneWin_Srv1 }}\"   \n        name        : \"{{ Win_Srv1 }}\"  \n        node        : \"{{ prox_node }}\"\n        storage     : \"{{ prox_storage }}\"\n        format      : \"{{ disk_format }}\"\n        timeout     : \"{{ timeout }}\"  \n    - name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win_Srv1 }}\"\n       ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - proxmox_kvm:\n        api_user    : \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host    : \"{{ apihost }}\"\n        clone       : \"{{ CloneWin_Srv2 }}\"   \n        name        : \"{{ Win_Srv2 }}\"  \n        node        : \"{{ prox_node }}\"\n        storage     : \"{{ prox_storage }}\"\n        format      : \"{{ disk_format }}\"\n        timeout     : \"{{ timeout }}\"  \n    - name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win_Srv2 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - proxmox_kvm:\n        api_user    : \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host    : \"{{ apihost }}\"\n        clone       : \"{{ CloneWin_10_1 }}\"  \n        name        : \"{{ Win_10_1 }}\"  \n        node        : \"{{ prox_node }}\"\n        storage     : \"{{ prox_storage }}\"\n        format      : \"{{ disk_format }}\"\n        timeout     : \"{{ timeout }}\"  \n    - name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win-10-1 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - proxmox_kvm:\n        api_user    : \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host    : \"{{ apihost }}\"\n        clone       : \"{{ CloneWin_10_2 }}\"   \n        name        : \"{{ Win_10_2 }}\"  \n        node        : \"{{ prox_node }}\"\n        storage     : \"{{ prox_storage }}\"\n        format      : \"{{ disk_format }}\"\n        timeout     : \"{{ timeout }}\"  \n     - name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win_10_2 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - proxmox_kvm:\n        api_user    : \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host    : \"{{ apihost }}\"\n        clone       : \"{{ CloneWin_10_3 }}\"   \n        name        : \"{{ Win_10_3 }}\"  \n        node        : \"{{ prox_node }}\"\n        storage     : \"{{ prox_storage }}\"\n        format      : \"{{ disk_format }}\"\n        timeout     : \"{{ timeout }}\" \n    - name: Pause for \"{{ pause }}\" seconds to clone the \"{{ Win_10_3 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - name: start \"{{ Win_Srv1 }}\"\n      proxmox_kvm:\n        api_user: \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host: \"{{ apihost }}\"\n        name: \"{{ Win_Srv1 }}\"\n        node: \"{{ prox_node }}\"\n        state: \"{{ state }}\"\n    - name: Pause for \"{{ pause }}\" seconds to start the \"{{ Win_Srv1 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - name: start \"{{ Win_Srv2 }}\"\n      proxmox_kvm:\n        api_user: \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host: \"{{ apihost }}\"\n        name: \"{{ Win_Srv2 }}\"\n        node: \"{{ prox_node }}\"\n        state: \"{{ state }}\"\n    - name: Pause for \"{{ pause }}\" seconds to start the \"{{ Win_Srv2 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - name: start \"{{ Win_10_1 }}\"\n      proxmox_kvm:\n        api_user: \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host: \"{{ apihost }}\"\n        name: \"{{ Win_10_1 }}\"\n        node: \"{{ prox_node }}\"\n        state: \"{{ state }}\"\n    - name: Pause for \"{{ pause }}\" seconds to start the \"{{ Win_10_1 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"\n    - name: start \"{{ Win_10_2 }}\"\n      proxmox_kvm:\n        api_user: \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host: \"{{ apihost }}\"\n        name: \"{{ Win_10_2 }}\"\n        node: \"{{ prox_node }}\"\n        state: \"{{ state }}\"\n    - name: Pause for \"{{ pause }}\" seconds to start the \"{{ Win_10_2 }}\"\n      ansible.builtin.pause:\n        seconds: \"{{ pause }}\"        \n    - name: start \"{{ Win_10_3 }}\"\n      proxmox_kvm:\n        api_user: \"{{ apiuser }}\"\n        api_password: \"{{ apipass }}\"\n        api_host: \"{{ apihost }}\"\n        name: \"{{ Win_10_3 }}\"\n        node: \"{{ prox_node }}\"\n        state: \"{{ state }}\"\n</code></pre></p>"},{"location":"ansible/ansible-playbook.html#lxc-container","title":"LXC Container","text":"Set Your Variables Fully made playbook for LXC Container"},{"location":"ansible/ansible-playbook.html#set-your-variables_1","title":"Set Your Variables","text":"Variables Functions Password Proxmox Password vmid The LXC Container id node The Proxmox Node to Deploy the LXC Container to api_user Proxmox Username, password and method (PAM or PVE) api_password Reused Proxmox Password from Password api_host Proxmox Server IP password The LXC Container Root Password hostname The Name of the LXC Container ostemplate The LXC Template downloaded to the Proxmox server cores The amount of cores assigned to the LXC Container memory The amount of RAM assinged to the LXC Container disk The amount of disk space assigned to the LXC Container storage The storage location for the LXC Container netif The network settings for the LXC Container nameserver The DNS server for the  LXC Container unprivileged Select if the LXC Container should be privileged or unprivileged validate_certs Select if the LXC Container should interact with HTTP and HTTPS web services onboot Select if the LXC Container should boot when the Proxmox server has booted description Sets the description for the LXC Container features Setup extra arguments"},{"location":"ansible/ansible-playbook.html#fully-made-playbook-for-lxc-container","title":"Fully made playbook for LXC Container","text":"<p>Please note:</p> <p>That all of our playbooks has pre-defined \"hosts\" in this case it is proxmox.</p> <p>This is because in our ansible hosts file our Proxmox server is named proxmox. <pre><code>- hosts: proxmox\n  tasks:\n  - set_fact:\n      Password: \"*\" # Password\n  - name: Create container for testing\n    community.general.proxmox:\n      vmid: \"5{{ lookup('password', '/dev/null chars=digits length=2') }}\"\n      node: main\n      api_user: root@pam\n      api_password: \"{{ Password }}\"\n      api_host: main.pve\n      password: \"{{ Password }}\"\n      hostname: \"test-{{ lookup('password', '/dev/null chars=ascii_lowercase,digits length=2') }}\"\n      ostemplate: 'local:vztmpl/ubuntu-22.04-standard_22.04-1_amd64.tar.zst'\n      cores: 2\n      memory: 4096\n      disk: \"40\"\n      storage: local-lvm\n      netif: '{\"net0\":\"name=eth0,gw=192.168.1.1,ip=dhcp,bridge=vmbr0\"}'\n      nameserver: 192.168.1.1\n      unprivileged: yes\n      validate_certs: no\n      onboot: yes\n      description: container for testing\n      features:\n       - nesting=1\n</code></pre></p>"},{"location":"ansible/ansible-setup.html","title":"Setup/Install Ansible","text":"Introduction <p>This is how I automated the deployment of a test environment, with 2 Windows Servers VMs as DC controllers and 3 Windows 10 VMs as client. All 3 clients have been sign into the Windows AD, and so have both the Windows Servers.</p> This Was Made for... <p>This was made for and deployed onto a Proxmox Server with ansible running on a Ubuntu VM from my Windows laptop as controller.</p>"},{"location":"ansible/ansible-setup.html#installation-on-ubuntu-vm","title":"Installation on Ubuntu VM","text":"Install sudo Install curl Install pip Install Ansible"},{"location":"ansible/ansible-setup.html#install-sudo","title":"Install sudo","text":"<p>This will be needed later for sudo nano. <pre><code>apt-get install sudo \n</code></pre></p>"},{"location":"ansible/ansible-setup.html#install-curl","title":"Install curl","text":"<p>This will be needed later to easier download the playbooks from github and install TailScale VPN <pre><code>sudo apt install curl\n</code></pre> Or <pre><code>sudo apt-get install curl\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#install-pip","title":"Install pip","text":"<p>This will be needed later to download/install ansible and it's modules and also update blowfish. <pre><code>sudo apt-get install python3 python3-pip -y\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#install-ansible","title":"Install Ansible","text":"<p>Install Ansible and make sure the everything is as it should. <pre><code>sudo pip3 install ansible \n</code></pre> Check that Ansible was installed with the command below. <pre><code>ansible --version\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#prep-proxmox-server","title":"Prep Proxmox Server","text":"Install Proxmoxer Update Blowfish"},{"location":"ansible/ansible-setup.html#install-proxmoxer","title":"Install Proxmoxer","text":"<p>This will be needed to deploy VMs via Ansible on the Proxmox server. This has to be done on the Proxmox server ifself. <pre><code>pip install proxmoxer\n</code></pre> <pre><code>pip install requests\n</code></pre> <pre><code>pip install paramiko\n</code></pre> <pre><code>pip install openssh_wrapper\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#update-blowfish","title":"Update Blowfish","text":"<p>Removes a warning when using ssh of Blowfish being deprecated.  (Do this to both Proxmox Server and Ubuntu VM) <pre><code>pip install blowfish\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#configuration-of-ansible-controller","title":"Configuration of Ansible controller","text":"Make Ansible.cfg <p>If you can't see any config file set for Ansible when you enter: <pre><code>Ansible --version\n</code></pre></p> <p>All you have to do is make a folder in <code>/etc</code></p> <p>You can do this with this command <pre><code>cd /etc\n\nsudo mkdir ansible\n</code></pre></p> <p>When you have made a ansible folder we can <code>cd</code> into it <pre><code>cd /etc/ansible\n</code></pre></p> <p>In this folder we need to make a file named ansible.cfg <pre><code>sudo nano ansible.cfg\n</code></pre></p> <p>When you get into the nano editer all you have to do is copy and paste the code from the link below or use this command.</p> <pre><code>cd /etc/ansible\nsudo wget https://gist.githubusercontent.com/alivx/2a4ca3e577ead4bd38d247c258e6513b/raw/fe2b9b1c7abc2b52cc6998525718c9a40c7e02a5/ansible.cfg\n</code></pre> <p>Github (Ansible.cfg)</p> Modify Hosts Modify Ansible.cfg Allow root login Push SSH key to nodes References"},{"location":"ansible/ansible-setup.html#modify-hosts","title":"Modify Hosts","text":"<p>After installing Ansible, the file /etc/hosts file is created automatically.  In this file, we are required to add our managed nodes.</p> <p>In the file, add your managed nodes as below.</p> <pre><code>[Node1]\n192.168.100.118\n[Node2]\n192.168.100.119\n</code></pre>"},{"location":"ansible/ansible-setup.html#modify-ansiblecfg","title":"Modify Ansible.cfg","text":"<p>Find and change the following settings in  <pre><code>sudo nano /etc/ansible/ansible.cfg\n</code></pre> <pre><code>inventory = /etc/ansible/hosts\n\nforks = 5\nsudo_user = root\n\nremote_user = root\n\nbecome=True\nbecome_method=sudo\nbecome_user=root\nbecome_ask_pass=False\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#allow-root-login","title":"Allow root login","text":"<p>(login to all nodes and do the following)</p> <p>To enable SSH login for a root user on Debian Linux system you need to first configure SSH server.  <pre><code>sudo nano /etc/ssh/sshd_config\n</code></pre></p> <p>Change the following line: <pre><code>FROM:\nPermitRootLogin without-password\n\nTO:\nPermitRootLogin yes\n</code></pre></p> <p>Once you made the above change restart your SSH server: <pre><code># /etc/init.d/ssh restart\n[ ok ] Restarting ssh (via systemctl): ssh.service.\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#push-ssh-key-to-nodes","title":"Push SSH key to nodes","text":"<p>This is done so that Ansible can use the SSH key to sign in as a user <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub your_username@192.168.100.118\n</code></pre></p>"},{"location":"ansible/ansible-setup.html#references","title":"References","text":"<ol> <li>Install and use ansible on debian linux</li> <li>Blowfish</li> <li>Proxmoxer</li> <li>Install pip on debian</li> <li>Install curl on debian</li> <li>Enable sudo user account debian</li> </ol>"},{"location":"ansible/ansible-use.html","title":"Use a Ansible-playbook","text":"Find the right folder Use SSH without having to type in the password Use your playbooks"},{"location":"ansible/ansible-use.html#find-the-right-folder","title":"Find the right folder","text":"<p>This will be the \"root\" folder for Ansible. <pre><code>cd /etc/ansible\n</code></pre></p>"},{"location":"ansible/ansible-use.html#use-ssh-without-having-to-type-in-the-password","title":"Use SSH without having to type in the password","text":"<p>First \"Read\" your SSH key. <pre><code>eval `ssh-agent`\n</code></pre> and then \"Load\" your SSH key with your password. <pre><code>ssh-add\n</code></pre></p>"},{"location":"ansible/ansible-use.html#use-your-playbooks","title":"Use your playbooks","text":"<p>First make sure you are in the same folder as the playbook you want to use.</p> <p>Then write: <pre><code>ansible-playbook Win-Test-Env.yaml\n</code></pre></p>"},{"location":"ansible/ansible.html","title":"What is Ansible?","text":"What is Ansible How much does it cost? How to Install Ansible"},{"location":"ansible/ansible.html#what-is-ansible_1","title":"What is Ansible?","text":"<p>Ansible\u00ae is an open source, command-line IT automation software application written in Python. </p> <p>It can configure systems, deploy software, and orchestrate advanced workflows to support application deployment, system updates, and more.</p> <p>Ansible\u2019s main strengths are simplicity and ease of use. </p> <p>It also has a strong focus on security and reliability, featuring minimal moving parts. </p> <p>It uses OpenSSH for transport (with other transports and pull modes as alternatives), and uses a human-readable language that is designed for getting started quickly without a lot of training.</p>"},{"location":"ansible/ansible.html#how-much-does-it-cost","title":"How much does it cost?","text":"<p>It is free.</p> <p>The community distribution of Ansible contains a suite of powerful command line tools supported on most operating systems with Python installed. </p> <p>This includes Red Hat\u00ae Enterprise Linux\u00ae, Debian, Ubuntu, MacOS, FreeBSD, Microsoft Windows, and more. For more information on installing Ansible refer to the installation documentation.</p>"},{"location":"ansible/ansible.html#how-to-install-ansible","title":"How to install Ansible?","text":"<p>Below you will find a button, clicking it will take you to our guide on how to get started with Ansible, with a laptop running Windows 10, that has the Ubuntu 22.04 VM from the Microsoft Store installed and configured as the Ansible Controller, that then controls an Proxmox Server with playbooks from the Ansible Controller.</p>"},{"location":"blog/blog.html","title":"Blog","text":""},{"location":"blog/blog.html#our-newest-blog-posts","title":"Our newest blog posts","text":"<ul> <li> <p>Power Efficient Home Server</p> </li> <li> <p>Setup Local Internet Security</p> </li> </ul>"},{"location":"blog/power-efficient-home-server.html","title":"Power Efficient Home Server","text":""},{"location":"blog/power-efficient-home-server.html#why-care-about-power-efficiency","title":"Why care about power efficiency?","text":"<p>Save Money</p> <p>Save money via running costs due to the server pulling less power from the wall, The server will output less heat and due to less heat the servers fans will not be running at 7200rpm, the server will therefore also run quietly.</p>"},{"location":"blog/power-efficient-home-server.html#platform-motherboard-cpu","title":"Platform (Motherboard &amp; CPU)","text":"<p>Motherboard &amp; CPU</p> <p>Building a power efficient server begins with, well, picking power efficient components. And that\u2019s not as simple as buying a low TDP CPU and pairing it with a server motherboard.</p> <p>First things first, if you want to save on your utility bill, don\u2019t buy old hardware, by old I mean pretty much anything older than Haswell or the 4<sup>th</sup> series of Intel CPUs.</p> <p>Older systems might be cheaper, but your savings will probably go towards paying the increased utility bill, even though older CPUs might have the same nominal TDP than the newer ones,they\u2019re much less efficient at idle and might not support the power efficient C-States that the newer CPUs support.</p> <p> </p> <p>Newer or older hardware?</p> <p>At the same time, newer doesn\u2019t necessarily mean \u201cmore power efficient\u201d, And recently we\u2019ve seen the PC components like CPUs and graphic cards Become more and more power hungry for the sake of \u2018performance\u2019. </p> <p>Pretty much any Intel CPU that is 6<sup>th</sup> gen or newer will be pretty efficient at idle, And even though the newer CPUs might be slightly more efficient, You will have to shell out more money for a newer motherboard and a more expensive CPU So if you\u2019re okay with the performance of something like 6<sup>th</sup> or 7<sup>th</sup> gen CPUs, don\u2019t chase the newer 12<sup>th</sup> and 13<sup>th</sup> CPUs purely because of the power efficiency.</p>"},{"location":"blog/power-efficient-home-server.html#but-what-about-amd","title":"But what about AMD?","text":"<p>Well, when it comes to Intel vs. AMD, Intel systems tend to win When it comes to budget-ish systems, Since they tend to consume less than equivalent AMD Ryzen machines.</p> <p></p> <p>Ryzen CPUs also still seem to suffer from the idle freeze bug on Linux, Which can only be fixed reilably by disabling the power efficient C-states, Which is kind of counter-intuitive, That being said, you can still build a pretty power efficient system with a Ryzen CPU, And some users report as little as 7W power consumption with a 4350G. </p> <p>One more thing I want to mention is that the TDP spec means absolutely nothing for the real-world power consumption in a home server. TDP only describes the power consumption under load, And in a lot of cases, even that figure doesn\u2019t match what you\u2019d see in real-life use. </p> <p>Despite consuming upwards of 100W under load, Many modern processors can still enter a power-efficient idle state In which they sip less than 1W.</p> <p></p> <p>That also applies to the T-Series Intel CPUs These are pretty much the same chips as the non-T models, just capped to a smaller TDP.</p> <p></p> <p>This has no effect on the idle power consumption, so don\u2019t pay more money for a T-series CPU because of the supposed power efficiency Since your server will most likely be idling most of the time,at least compared to a desktop PC.</p> <p> </p> <p>The idle draw is exactly the figure we\u2019re interested in and it can vary wildly depending on your motherboard, PCIe devices, power supply, and so on. </p> <p>Unfortunately, very few manufacturers and reviewers publish the idle power consumption figures, but luckily for us, there\u2019s a whole community focused on building power efficient computers. </p> <p>And it can be found on this German forum called Hardwareluxx. The forum members even maintain a database of the most power efficient builds, sorted by consumption which can be a great help if you\u2019re looking for power efficient components</p>"},{"location":"blog/power-efficient-home-server.html#ultra-small-form-factor-pcs","title":"Ultra small-form-factor PCs","text":"<p>So let\u2019s dive in As you can see, the most power efficient systems, ones that consume as little as 1 to 4 watts, are Intel NUCs, laptop motherboards and basically ultra small form factor PCs. </p> <p>Obviously, those computers are power efficient for a reason they don\u2019t have the most beefy CPUs in them And the number of features and ports is also very limited. </p> <p>You\u2019ll rarely find more than one SATA or M.2 slot, and PCIe is also usually out of the question. That\u2019s the price you\u2019ll have to pay for ultimate power efficiency, and if you\u2019re okay with not having those features, then these options could be for you. </p> <p>These machines should still have plenty of power for Virtualization, Docker, Kubernetes, Proxmox, or even running a media server or a Home Assistant Instance But they\u2019re probably not the best choice for a NAS since they lack expansion.</p> <p></p>"},{"location":"blog/power-efficient-home-server.html#miniitx-motherboards","title":"miniITX Motherboards","text":"<p>If you want your home server to be a bit more capable, The next option is a desktop CPU and a miniITX motherboard MiniITX motherboards tend to have less ports and features than their ATX counterparts, and because of that they usually consume less power You might think that the difference between a miniITX motherboard and a full ATX mobo with the same CPU is negligible, Aaaand you\u2019d be wrong.</p> <p> </p>"},{"location":"blog/setup-local-internet-security.html","title":"Coming Soon","text":""},{"location":"blog/setup-local-internet-security.html#coming-soon_1","title":"Coming Soon","text":""},{"location":"cloud/getting-started.html","title":"Getting Started","text":""},{"location":"cloud/getting-started.html#a-guide-for-the-use-of-a-cloud-rp-helpdesk-vps","title":"A guide for the use of a Cloud RP-Helpdesk VPS","text":"Where to get a Cloud RP-Helpdesk VPS? Access VPS Access SFTP"},{"location":"cloud/getting-started.html#where-to-get-a-cloud-rp-helpdesk-vps","title":"Where to get a Cloud RP-Helpdesk VPS?","text":"<p>https://cloud.rp-helpdesk.com/</p>"},{"location":"cloud/getting-started.html#access-vps","title":"Access VPS","text":"<p>In order to access your VPS, you will have to sign up at tailscale to use their VPN software. https://login.tailscale.com/start</p> <p>When you have made an account, you will have to generate an auth key and send it to us, so we can  assign your VPS to your tailscale account. The auth key can be generate here</p> <p>https://login.tailscale.com/admin/settings/keys</p>"},{"location":"cloud/getting-started.html#access-sftp","title":"Access SFTP","text":"<p>Bitvise SSH Client sftp Access</p> <p></p> <ol> <li>Input Tailscale ip into the host field </li> <li>Keep the port on 22</li> <li>Input the username into the username field</li> <li>Log in</li> </ol> <p></p> <ol> <li>When successfully logged in </li> <li>Click the \u201dNew SFTP window\"</li> </ol>"},{"location":"debian/starship.html","title":"Starship","text":"Install Starship Setup Shell <p>Add the following to the end of <code>~/.bashrc</code>: <pre><code>eval \"$(starship init bash)\"\n</code></pre></p>"},{"location":"debian/starship.html#setting-up-tailscale-on-linux","title":"Setting up Tailscale on Linux","text":"<p>Tailscale works on a variety of Linux distributions. In general, you can install Tailscale on a Linux machine with a single command:</p> <pre><code>curl -sS https://starship.rs/install.sh | sh\n</code></pre>"},{"location":"debian/tailscale.html","title":"Tailscale on Debian","text":"Setting up Tailscale on Linux Ubuntu Debian CentOS openSUSE Oracle Linux Red Hat\u00ae Enterprise Linux"},{"location":"debian/tailscale.html#setting-up-tailscale-on-linux","title":"Setting up Tailscale on Linux","text":"<p>Tailscale works on a variety of Linux distributions. In general, you can install Tailscale on a Linux machine with a single command:</p> <pre><code>curl -fsSL https://tailscale.com/install.sh | sh\n</code></pre>"},{"location":"debian/tailscale.html#ubuntu","title":"Ubuntu","text":"<ul> <li>Ubuntu 16.04 LTS (xenial)</li> <li>Ubuntu 18.04 LTS (bionic)</li> <li>Ubuntu 19.10 (eoan)</li> <li>Ubuntu 20.04 LTS (focal)</li> <li>Ubuntu 20.10 (groovy)</li> <li>Ubuntu 21.04 (hirsute)</li> <li>Ubuntu 21.10 (impish)</li> <li>Ubuntu 22.04 (jammy)</li> </ul>"},{"location":"debian/tailscale.html#debian","title":"Debian","text":"<ul> <li>Debian Bookworm (testing)</li> <li>Debian Bullseye (stable)</li> <li>Debian Buster (oldstable)</li> <li>Debian Sid (unstable)</li> <li>Debian Stretch (oldoldstable)</li> <li>Raspbian Bullseye (for Raspberry Pi)</li> <li>Raspbian Buster (for Raspberry Pi)</li> <li>Raspbian Stretch (for Raspberry Pi)</li> </ul>"},{"location":"debian/tailscale.html#centos","title":"CentOS","text":"<ul> <li>CentOS 7</li> <li>CentOS 8</li> <li>CentOS Stream 9</li> </ul>"},{"location":"debian/tailscale.html#opensuse","title":"openSUSE","text":"<ul> <li>openSUSE Leap 15.1</li> <li>openSUSE Leap 15.2</li> <li>openSUSE Tumbleweed</li> </ul>"},{"location":"debian/tailscale.html#oracle-linux","title":"Oracle Linux","text":"<ul> <li>Oracle Linux 7</li> <li>Oracle Linux 8</li> </ul>"},{"location":"debian/tailscale.html#red-hat-enterprise-linux","title":"Red Hat\u00ae Enterprise Linux","text":"<ul> <li>RHEL 8</li> <li> <p>RHEL 9</p> </li> <li> <p>Fedora</p> </li> <li>Amazon Linux 2</li> <li>Arch Linux</li> <li>NixOS</li> <li>Static binaries (for unsupported OSes)</li> </ul>"},{"location":"docker/cloudflared-docker-setup.html","title":"Cloudflared Docker","text":"<p>This is a short guide for setting up a cloudflare tunnel with a docker container.</p>"},{"location":"docker/cloudflared-docker-setup.html#docker-setup","title":"Docker Setup","text":"<p>All information needed for installing the docker itself.</p> Create App Folder Authorise Cloudflared Create a tunnel Create the config.yaml Start the cloudflared docker"},{"location":"docker/cloudflared-docker-setup.html#create-app-folder","title":"Create App Folder","text":"<p>First we need to make sure we have the app folder ready with the correct permissions. you can use the following command in the terminal to create the folder and set the correct permissions:</p> <pre><code>mkdir -p /mnt/user/appdata/cloudflared/ &amp;&amp; chmod -R 777 /mnt/user/appdata/cloudflared/\n</code></pre>"},{"location":"docker/cloudflared-docker-setup.html#authorise-cloudflared","title":"Authorise Cloudflared","text":"<p>Run the following command to authorize Cloudflared with the Cloudflare site you want to set up with a tunnel.</p> <pre><code>docker run -it --rm -v /mnt/user/appdata/cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel login\n</code></pre> <p>It will print out a link to Cloudflare. Put this link in your web browser, and select which domain you want to use. Then, the daemon will automatically pull the certificate.</p>"},{"location":"docker/cloudflared-docker-setup.html#create-a-tunnel","title":"Create a tunnel","text":"<p>Now we need to create a tunnel. To do this, we will run another command from the terminal:</p> <pre><code>docker run -it --rm -v /mnt/user/appdata/cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel create TUNNELNAME\n</code></pre> <p>This will create your tunnel's UUID.json file, which contains a secret used to authenticate your tunnelled connection with Cloudflare. The JSON file is only needed for running the tunnel, but any tunnel modifications require the cert.pem. More information about what requires what can be found here.</p> <p>Make sure you copy your UUID, as this will be used in later steps. It can always be found later by the name of the JSON file.</p>"},{"location":"docker/cloudflared-docker-setup.html#create-the-configyaml","title":"Create the config.yaml","text":"<p>Now we need to create a config.yaml to configure the tunnel</p> <pre><code>nano /mnt/user/appdata/cloudflared/config.yml\n</code></pre> <p>Now paste in the following and amend your reverse proxy IP:PORT, tunnel UUID and domain name if applicable</p> <ul> <li>if you have an ssl certificate on your reverse proxy, you need to pass in your domain name that the SSL cert is under</li> <li>if you want to proxy to an http server, use the commended ingress rule</li> <li>if you want to disable ssl verification, add noTLSVerify under originRequest</li> </ul> <pre><code>tunnel: UUID\ncredentials-file: /home/nonroot/.cloudflared/UUID.json\n\n# NOTE: You should only have one ingress tag, so if you uncomment one block comment the others\n\n# forward all traffic to Reverse Proxy w/ SSL\ningress:\n  - service: https://REVERSEPROXYIP:PORT\n    originRequest:\n      originServerName: yourdomain.com\n\n#forward all traffic to Reverse Proxy w/ SSL and no TLS Verify\n#ingress:\n#  - service: https://REVERSEPROXYIP:PORT\n#    originRequest:\n#      noTLSVerify: true\n\n# forward all traffic to reverse proxy over http\n#ingress:\n#  - service: http://REVERSEPROXYIP:PORT\n</code></pre>"},{"location":"docker/cloudflared-docker-setup.html#start-the-cloudflared-docker","title":"Start the cloudflared docker","text":"<p>Start the cloudflare docker container with the following command:</p> <pre><code>docker run -it -d -v /mnt/user/appdata/cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel run -- UUID\n</code></pre>"},{"location":"docker/docker-customcompose.html","title":"Custom Docker-compose","text":""},{"location":"docker/docker-customcompose.html#portainer-templates","title":"Portainer Templates","text":"Seafile Vaultwarden Flamedashboard Vikunja"},{"location":"docker/docker-customcompose.html#seafile","title":"Seafile","text":"<p>Self-hosted onedrive.</p> <p>This can be used together with a Windows 10 application that will make seafile show up in the file explorer like onedrive does.</p> <pre><code>version: '2.0'\nservices:\n  db:\n    image: mariadb\n    container_name: seafile-mysql\n    environment:\n      - MYSQL_ROOT_PASSWORD=*\n      - MYSQL_LOG_CONSOLE=true\n    volumes:\n      - ./data/mariadb:/var/lib/mysql\n    networks:\n      - seafile-net\n\n  memcached:\n    image: memcached\n    container_name: seafile-memcached\n    entrypoint: memcached -m 256\n    networks:\n       - seafile-net\n\n  seafile:\n    image: seafileltd/seafile-mc\n    container_name: seafile\n    ports:\n      - \"8080:80\"\n    volumes:\n      - ./data/app:/shared\n    environment:\n      - DB_HOST=db\n      - DB_ROOT_PASSWD=*\n      - TIME_ZONE=Etc/UTC\n      - SEAFILE_ADMIN_EMAIL=*\n      - SEAFILE_ADMIN_PASSWORD=*\n      - SEAFILE_SERVER_LETSENCRYPT=false\n      - SEAFILE_SERVER_HOSTNAME=*\n    depends_on:\n      - db\n      - memcached\n    networks:\n      - seafile-net\n\nnetworks:\n  seafile-net:\n</code></pre>"},{"location":"docker/docker-customcompose.html#vaultwarden","title":"Vaultwarden","text":"<p>Self-hosted password manager.</p> <p>This can use all of bitwardens applications on IOS, Android, Windows, Mac, Chrome, Firefox, etc. <pre><code>version: '3.4'\nservices:\n  vaultwarden:\n    image: vaultwarden/server:latest\n    environment:\n      - ADMIN_TOKEN=*\n      - SIGNUPS_ALLOWED=false\n    volumes:\n      - ./vw_data:/data\n    ports:\n      - 17881:80\n</code></pre></p>"},{"location":"docker/docker-customcompose.html#flamedashboard","title":"Flamedashboard","text":"<p>Self-hosted dashboard</p> <p>This can be used for quick access to all of your self-hosted services. <pre><code>version: '2.1'\nservices:\n  flame:\n    image: pawelmalak/flame:latest\n    container_name: flame\n    volumes:\n      - /srv/config/flame:/app/data\n    ports:\n      - 5005:5005\n    environment:\n      - PASSWORD=*\n    restart: unless-stopped\n</code></pre></p>"},{"location":"docker/docker-customcompose.html#vikunja","title":"Vikunja","text":"<p>Self-hosted to do list manager</p> <pre><code>version: '3'\n\nservices:\n  db:\n    image: mariadb:10\n    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci\n    environment:\n      MYSQL_ROOT_PASSWORD: supersecret\n      MYSQL_USER: vikunja\n      MYSQL_PASSWORD: secret\n      MYSQL_DATABASE: vikunja\n    volumes:\n      - ./db:/var/lib/mysql\n    restart: unless-stopped\n  api:\n    image: vikunja/api\n    environment:\n      VIKUNJA_DATABASE_HOST: db\n      VIKUNJA_DATABASE_PASSWORD: secret\n      VIKUNJA_DATABASE_TYPE: mysql\n      VIKUNJA_DATABASE_USER: vikunja\n      VIKUNJA_DATABASE_DATABASE: vikunja\n      VIKUNJA_SERVICE_JWTSECRET: &lt;a super secure random secret&gt;\n      VIKUNJA_SERVICE_FRONTENDURL: https://&lt;your public frontend url with slash&gt;/\n    ports:\n      - 3456:3456\n    volumes:\n      - ./files:/app/vikunja/files\n    depends_on:\n      - db\n    restart: unless-stopped\n  frontend:\n    image: vikunja/frontend\n    ports:\n      - 4321:80\n    environment:\n      VIKUNJA_API_URL: http://vikunja-api-domain.tld/api/v1\n    restart: unless-stopped\n</code></pre>"},{"location":"docker/docker.html","title":"What is Docker?","text":"Docker Overview The Docker platform What can I use Docker for?"},{"location":"docker/docker.html#docker-overview","title":"Docker overview","text":"<p>Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production.</p>"},{"location":"docker/docker.html#the-docker-platform","title":"The Docker platform","text":"<p>Docker provides the ability to package and run an application in a loosely isolated environment called a container. The isolation and security allows you to run many containers simultaneously on a given host. Containers are lightweight and contain everything needed to run the application, so you do not need to rely on what is currently installed on the host. You can easily share containers while you work, and be sure that everyone you share with gets the same container that works in the same way.</p> <p>Docker provides tooling and a platform to manage the lifecycle of your containers:</p> <ul> <li>Develop your application and its supporting components using containers.</li> <li>The container becomes the unit for distributing and testing your application.</li> <li>When you\u2019re ready, deploy your application into your production environment, as a container or an orchestrated service. This works the same whether your production environment is a local data center, a cloud provider, or a hybrid of the two.</li> </ul>"},{"location":"docker/docker.html#what-can-i-use-docker-for","title":"What can I use Docker for?","text":"<p>Fast, consistent delivery of your applications</p> <p>Docker streamlines the development lifecycle by allowing developers to work in standardized environments using local containers which provide your applications and services. Containers are great for continuous integration and continuous delivery (CI/CD) workflows.</p> <p>Consider the following example scenario:</p> <ul> <li>Your developers write code locally and share their work with their colleagues using Docker containers. </li> <li>They use Docker to push their applications into a test environment and execute automated and manual tests.</li> <li>When developers find bugs, they can fix them in the development environment and redeploy them to the test environment for testing and validation.</li> <li>When testing is complete, getting the fix to the customer is as simple as pushing the updated image to the production environment.</li> </ul> <p>Responsive deployment and scaling</p> <p>Docker\u2019s container-based platform allows for highly portable workloads. Docker containers can run on a developer\u2019s local laptop, on physical or virtual machines in a data center, on cloud providers, or in a mixture of environments.</p> <p>Docker\u2019s portability and lightweight nature also make it easy to dynamically manage workloads, scaling up or tearing down applications and services as business needs dictate, in near real time.</p> <p>Running more workloads on the same hardware</p> <p>Docker is lightweight and fast. It provides a viable, cost-effective alternative to hypervisor-based virtual machines, so you can use more of your server capacity to achieve your business goals. Docker is perfect for high density environments and for small and medium deployments where you need to do more with fewer resources.</p>"},{"location":"policy/pw-policy.html","title":"Policy","text":""},{"location":"portainer/portainer-setup.html","title":"Setup Portainer/Docker On Debain 11","text":"Prerequisites Update and Upgrade Install Docker CE Install Docker Compose Install Portainer UI Access Portainer UI Conclusion"},{"location":"portainer/portainer-setup.html#prerequisites","title":"Prerequisites","text":"<ul> <li>A server running Debian 11 </li> <li>A root password configured on your server</li> </ul>"},{"location":"portainer/portainer-setup.html#update-and-upgrade","title":"Update and Upgrade","text":"<p>Update and Upgrade the debain 11 \"server\" to get the newest security updates. And to make sure that all repos are up to date.</p> <pre><code>apt-get update -y\n\napt-get upgrade -y\n</code></pre>"},{"location":"portainer/portainer-setup.html#install-docker-ce","title":"Install Docker CE","text":"<p>Before installing Portainer, Docker CE must be installed on your server. You can install Docker CE by following the below steps:</p> <p>First, install the required dependencies using the following command:</p> <pre><code>apt-get install apt-transport-https ca-certificates curl gnupg2 software-properties-common\n</code></pre> <p>Next, add the Docker CE repository with the following command:</p> <pre><code>curl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\necho \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n$(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre> <p>Next, update the repository and install Docker CE with the following command:</p> <pre><code>apt-get update -y\napt-get install docker-ce docker-ce-cli containerd.io -y\n</code></pre> <p>Once Docker is installed, verify the Docker installation using the following command:</p> <pre><code>docker version\n</code></pre> <p>You should see the following output:</p> <pre><code>Client: Docker Engine - Community\n Version:           20.10.12\n API version:       1.41\n Go version:        go1.16.12\n Git commit:        e91ed57\n Built:             Mon Dec 13 11:45:48 2021\n OS/Arch:           linux/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Engine - Community\n Engine:\n  Version:          20.10.12\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.16.12\n  Git commit:       459d0df\n  Built:            Mon Dec 13 11:43:56 2021\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.12\n  GitCommit:        7b11cfaabd73bb80907dd23182b9347b4245eb5d\n runc:\n  Version:          1.0.2\n  GitCommit:        v1.0.2-0-g52b36a2\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n</code></pre>"},{"location":"portainer/portainer-setup.html#install-docker-compose","title":"Install Docker Compose","text":"<p>You will also need to install Docker Compose to your system. First, download the latest version of Docker Compose binary using the following command:</p> <pre><code>wget https://github.com/docker/compose/releases/download/v2.2.2/docker-compose-linux-x86_64\n</code></pre> <p>Next, copy the downloaded binary to the system path:</p> <pre><code>cp docker-compose-linux-x86_64 /usr/local/bin/docker-compose\n</code></pre> <p>Next, set executable permissions to the Docker Compose binary:</p> <pre><code>chmod +x /usr/local/bin/docker-compose\n</code></pre> <p>Next, verify the Docker Compose installation using the following command:</p> <pre><code>docker-compose --version \n</code></pre> <p>You should see the following output: <pre><code>Docker Compose version v2.2.2\n</code></pre></p>"},{"location":"portainer/portainer-setup.html#install-portainer-ui","title":"Install Portainer UI","text":"<p>First, create a volume to store Portainer data using the following command:</p> <pre><code>docker volume create portainer_data\n</code></pre> <p>Next, run the following command to download the Portainer image from the Docker Hub registry, create a container, and expose the container on port 9000:</p> <pre><code>docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer\n</code></pre> <p>You should see the following output:</p> <pre><code>Unable to find image 'portainer/portainer:latest' locally\nlatest: Pulling from portainer/portainer\n94cfa856b2b1: Pull complete \n49d59ee0881a: Pull complete \na2300fd28637: Pull complete \nDigest: sha256:fb45b43738646048a0a0cc74fcee2865b69efde857e710126084ee5de9be0f3f\nStatus: Downloaded newer image for portainer/portainer:latest\n69d06a5c41851cc85df1924aa77566d2f38978faad5c27f52e089af8a0dc931e\n</code></pre> <p>You can verify the running container using the following command:</p> <pre><code>docker ps\n</code></pre> <p>You should see the following output:</p> <pre><code>CONTAINER ID   IMAGE                 COMMAND        CREATED          STATUS          PORTS                                                                                  NAMES\n69d06a5c4185   portainer/portainer   \"/portainer\"   12 seconds ago   Up 11 seconds   0.0.0.0:8000-&gt;8000/tcp, :::8000-&gt;8000/tcp, 0.0.0.0:9000-&gt;9000/tcp, :::9000-&gt;9000/tcp   portainer\n</code></pre>"},{"location":"portainer/portainer-setup.html#access-portainer-ui","title":"Access Portainer UI","text":"<p>Now, open your web browser and access the Portainer UI using the URL http://your-server-ip:9000. You should see the following page:</p> <p></p> <p>Set your admin user and password and click on the Create user button. You will be asked to select the Docker environment that you want to manage:</p> <p></p> <p>Select the local environment and click on the Connect button. You should see the Portainer UI on the following page:</p> <p></p>"},{"location":"portainer/portainer-setup.html#conclusion","title":"Conclusion","text":"<p>Congratulations! You have successfully installed Portainer UI on Debian 11. You can now create, deploy and manage Docker containers from a web browser.</p>"},{"location":"portainer/portainer.html","title":"What is Portainer?","text":"Portainer <p>Portainer is a lightweight management UI which allows you to easily manage your different Docker environments (Docker hosts or Swarm clusters). Portainer is meant to be as simple to deploy as it is to use. </p> <p>It consists of a single container that can run on any Docker engine (can be deployed as Linux container or a Windows native container, supports other platforms too). </p> <p>Portainer allows you to manage all your Docker resources (containers, images, volumes, networks and more) ! It is compatible with the standalone Docker engine and with Docker Swarm mode.</p>"},{"location":"portainer/portainer.html#top-features","title":"Top Features","text":"Authentication Manage multiple clusters Template View Docker Resources Perform Actions On Containers"},{"location":"portainer/portainer.html#authentication","title":"Authentication","text":"<p>Portainer supports three different methods for user authentications. The first is the internal method, where the user management is handled by Portainer and stored in the Portainer data folder. </p> <p>The second method is LDAP, where the users are managed by an external LDAP server, such as ActiveDirectory. The only drawback that I can see for the LDAP authentication is that it cannot be configured using environment variables. </p> <p>The final authentication method is to use OAuth to perform the authentication. (This feature requires purchasing one of the paid extensions.)</p>"},{"location":"portainer/portainer.html#manage-multiple-clusters-endpoints","title":"Manage multiple clusters (endpoints)","text":"<p>As shown in the image below, Portainer supports managing multiple Docker Swarm clusters (called endpoints) using its user interface. </p> <p>This feature is very helpful in case you would like to manage clusters for different environments (such as testing, staging, and production) in a single user interface.</p>"},{"location":"portainer/portainer.html#template","title":"Template","text":"<p>Portainer allows the end user to deploy common Docker Swarm services or Docker containers from predefined templates. These templates are either shipped with Portainer itself or can be defined by the end users themselves. The user-defined templates can be used to deploy services using three modes:</p> <ul> <li>Single Docker containers \u2014 by defining all the needed configurations to create the container</li> <li>Docker Swarm services \u2014 by providing a link to the stack or service file</li> <li>Docker containers \u2014 using a docker-compose file</li> </ul>"},{"location":"portainer/portainer.html#view-docker-resources","title":"View Docker Resources","text":"<p>Another important feature provided by Portainer is the ability to view, interact, and manage Docker Swarm resources such as Swarm service, Docker networks, Swarm stacks, containers, and Docker images.</p> <p>Portainer allows the end user to view these resources and interact with them in a way that is easy, intuitive, and simple. As a result, the end user does not need to connect to the Swarm clusters and move from one node to another to validate and check the services and the containers. </p> <p>For instance, the end user can list all the services deployed to a given cluster, update the configuration items of these services, and stop and start the services as needed from the web interface.</p>"},{"location":"portainer/portainer.html#perform-actions-on-containers","title":"Perform Actions On Containers","text":"<p>This feature is highly related to the previous one, but since the actions that can be performed on the containers are super important and helpful, I want to highlight these features.</p> <p>Usually in Docker Swarm clusters, if an engineer would like to inspect a given container, check the container logs, or connect to the shell inside the Docker container, they need to perform the following actions. </p> <p>First, connect to a manager node. Then check where are the containers of the service are deployed (in which worker node). After that, connect to the worker node and perform the action needed on the container.</p> <p>All these actions are not needed when hosting Portainer. The same goal can be achieved directly from the web interface, and there is no need to connect to any cluster node.</p>"},{"location":"portainer/portainer.html#paid-extensions","title":"Paid Extensions","text":"Paid Extensions <p>Portainer can be extended by purchasing and enabling extra plugins. The list of the available Portainer plugins is described below.</p> Registry Manager extension Role-Based Access Control extension External Authentication extension"},{"location":"portainer/portainer.html#registry-manager-extension","title":"Registry Manager extension","text":"<p>This plugin extends Portainer and provides the end users with the capability to browse the defined external Docker registries and manage their resources. </p> <p>Once the plugin is installed and enabled, users are able to browse the registries, explore repositories and Docker images, manipulate the tags that are attached to the repositories and images, add new tags, retag an image, and delete existing tags.</p> <p>The Registry Manager plugin has a simple and intuitive GUI that makes managing the Docker registry and the Swarm clusters easier and can be done from a single interface. On the other hand, there is still room for improvement in the plugin. For instance, the plugin does not support automating the Docker registry clean-up and the removal of old and unneeded Docker images. </p> <p>This feature is very important if the Docker registry is used in the development process and engineers are building Docker images from the feature branches of the applications. In that case, it would be nice to have a feature in the plugin to remove all images that belong to the feature branches after 30 days.</p>"},{"location":"portainer/portainer.html#role-based-access-control-extension","title":"Role-Based Access Control extension","text":"<p>The Role-Based Access Control extension (RBAC) extends Portainer authorization features and allows the end user to assign roles to users or teams within an endpoint or group of endpoints. There are multiple different predefined roles that are available once the plugin is installed and enabled. These roles are described below.</p> <ul> <li>Endpoint administrator: This role provides complete control over the resources deployed within a given endpoint (only the resources and Portainer internal settings are not included).</li> <li>Standard user: This role has complete control over the resources that are deployed by the user himself or by one of the members of the team that the user belongs to.</li> <li>Helpdesk: This role provides read-only permissions to all the resources within the selected endpoint.</li> <li>Read-only user: This role provides read-only permissions to a subset of the resources within the selected endpoint.</li> </ul> <p>In addition to the above roles, Portainer defines a built-in role of Administrator that grants access permission not only to an endpoint resource but also to the Portainer settings and all the resources in all defined endpoints.</p> <p>In my opinion, this plugin is a good addition to the Portainer web interface, However, I am missing the following features that could improve this plugin further.</p> <ul> <li>The ability to create custom roles</li> <li>The ability to grant users manage permissions to a subset of the resources within an endpoint</li> </ul>"},{"location":"portainer/portainer.html#external-authentication-extension","title":"External Authentication extension","text":"<p>This plugin can be used to extend the authentication methods supported by Portainer. In case the internal user management is considered to be too much work and the LDAP server is not available, this plugin can be used to manage the user authentication using an OAuth service.</p> <p>Once the plugin is installed and enabled, an additional authentication option within Portainer settings will be available. The OAuth option adds the ability to connect Portainer to Microsoft Azure AD, Google Suite/Cloud Authentication, and GitHub Authentication. Furthermore, the plugin allows using the custom option, where you can connect to any OAuth provider by imputing the correct URI fields.</p>"},{"location":"proxmox/our-repository.html","title":"Setup Repository","text":"Setup Repository Proxmox Cluster Setup Repository Proxmox Backup Server"},{"location":"proxmox/our-repository.html#setup-repository-for-proxmox-cluster-with-mixed-intel-cpu-gens","title":"Setup Repository For Proxmox Cluster With Mixed Intel CPU Gen's","text":"<p>Start by editing the apt source list</p> <p><pre><code>nano /etc/apt/sources.list\n</code></pre> In the file you can delete anything already there and replace it with the lines below:</p> <pre><code>deb http://ftp.dk.debian.org/debian bullseye main contrib\n\ndeb http://ftp.dk.debian.org/debian bullseye-updates main contrib\n\n# security updates\ndeb http://security.debian.org bullseye-security main contrib\n\ndeb http://ftp.debian.org/debian bullseye main contrib\ndeb http://ftp.debian.org/debian bullseye-updates main contrib\n\n# PVE pve-no-subscription repository provided by proxmox.com,\n# NOT recommended for production use\ndeb http://download.proxmox.com/debian/pve bullseye pve-no-subscription\n\n# security updates\ndeb http://security.debian.org/debian-security bullseye-security main contrib\n</code></pre>"},{"location":"proxmox/our-repository.html#setup-repository-proxmox-backup-server","title":"Setup Repository Proxmox Backup Server","text":"<pre><code># echo \"deb http://download.proxmox.com/debian/pbs buster pbs-no-subscription\" &gt; /etc/apt/sources.list.d/pbs-community.list\n# apt update\n# apt -y full-upgrade\n</code></pre>"},{"location":"proxmox/proxmox-errors.html","title":"Proxmox Errors","text":"<p>These are the error that I have found within Proxmox.</p> <p></p>"},{"location":"proxmox/proxmox-errors.html#vm-errors","title":"VM Errors","text":"Windows ISO Fail to boot Resolved by <p>Install/enable QEMU Guest Agent in Proxmox VM wizard before booting the VM for the first time</p> Windows 10 Template Blue Screen Resolved by <p>If you don\u2019t care about the CD/DVD drive, just leave it be. The missing drive is what is causing Windows to blue screen. If you have already remove the drive, just re-added it again but make sure it is the right ISO you pick.</p> Proxmox Virtual Environment 7.2-11 Live Migration Resolved by <p>Installing pve-kernel-5.19 repository <pre><code>apt install pve-kernel-5.19\n</code></pre></p> <p>After restarting both PVE servers they are running on pve-kernel-5.19.7-1-pve and now</p> <p>When migrating Xeon(R) CPU E5-2650 v2 \u2192&gt; Xeon(R) Bronze 3106 </p> <p>\u2192 Everything is ok, VM works and does not freeze</p> <p>When migrating Xeon(R) Bronze 3106 \u2192&gt; Xeon(R) CPU E5-2650 v2 </p> <p>\u2192 Everything is ok, VM is running and not freezing</p>"},{"location":"proxmox/proxmox-errors.html#windows-iso-fail-to-boot","title":"Windows ISO Fail to boot","text":"<p>This error seems to have something to do with, not having QEMU Guest Agent installed/enabled from the Proxmox VM wizard.</p>"},{"location":"proxmox/proxmox-errors.html#windows-10-template-blue-screen","title":"Windows 10 Template Blue Screen","text":"<p>This error has been \u201ccreated\u201d by making a VM, that runs an completely fresh installation of windows 10, without any updates or software installed. </p> <p>Then powering the VM down and making it to a Proxmox template. When it has been made into a template then remove the CD/DVD drive. </p>"},{"location":"proxmox/proxmox-errors.html#proxmox-virtual-environment-72-11-live-migration","title":"Proxmox Virtual Environment 7.2-11 Live Migration","text":"<p>This problem have been covered by Proxmox's own forum.</p> <p>Xeon(R) Bronze 3106 &lt;&lt;---&gt;&gt; Xeon(R) CPU E5-2650 v2 kernel 5.15</p> <p>When migrating Xeon(R) CPU E5-2650 v2 \u2192&gt; Xeon(R) Bronze 3106 </p> <p>\u2192 Everything is ok, VM is working</p> <p>When migrating Xeon(R) Bronze 3106 \u2192&gt; Xeon(R) CPU E5-2650 v2 </p> <p>\u2192 VM freezes, requires full VM shutdown and power on</p>"},{"location":"proxmox/proxmox-errors.html#vm-internet-access","title":"VM Internet Access","text":"Tailscale VPN On a Proxmox LXC Container Resolved by <p>This solution will fix any VPN you want to run such as Tailscale and OpenVPN <pre><code>cd /etc/pve/lxc\n</code></pre></p> <p>Find the vmid of the LXC Container and then copy the lines below into the LXC Containers config. (Just put it below the lines already there)</p> <pre><code>lxc.cgroup.devices.allow: c 10:200 rwm \nlxc.mount.entry: /dev/net/tun dev/net/tun none bind,create=file\n</code></pre> <p>AND DON'T FORGET TO REBOOT THE LXC CONTAINER AFTER...</p>"},{"location":"proxmox/proxmox-errors.html#tailscale-vpn-on-proxmox-lxc-container","title":"Tailscale VPN On Proxmox LXC Container","text":"<p>This issue has been found when trying to use tailscale on a Proxmox LXC Container. </p> <p>Tailscale can be installed without any problems, but when you try to run the <code>Sudo tailscale up</code> command it will return and error saying something along the lines of tailscale is not running try to.... and no matter what you try on the VM it will not be able to start tailscale.</p>"},{"location":"proxmox/proxmox-errors.html#proxmox-node-nic-error","title":"Proxmox Node NIC Error","text":"NIC Error <p>I just went through this issue and thought I should share. This seems more like a motherboard issue than a proxmox/networking one but I'm pretty inexperienced w/ linux.</p> <p>TL/DR: adding second nvme drive to server caused the network interface names to change by +1 each making the webui unreachable - fix by changing the names in /etc/network/interfaces and restarting the network service.</p> <p>Bug: I have 2 nvme drives that were purchased at different times. One already in operation w/ the system and the other to be installed. When the second nvme drive was installed the webui would be unreachable. Did not matter in which order the nvme drives were placed, it would still mess up.</p> Resolved by <p>Fix: Viewed my network interfaces: <pre><code>$ nano /etc/network/interfaces\n</code></pre></p> <p>took note that the two interface names were enp39s0 and enp45s0 and the vmbro0 was pointing to enp45s0 then ran these two commands to view interface status: <pre><code>$ ip -br -c link show\n$ ip -br -c addr show\n</code></pre></p> <p>When these were ran, it showed the interface names as enp40s0 and enp46s0. I then went back to /etc/network/interfaces and modified the file w/ the new info; so enp39s0 and enp45s0 and the vmbro0 pointing to enp45s0 \u2192 enp40s0 and enp46s0 and the vmbro0 pointing to enp46s0 now</p> <p>I then flushed the ip info on all interfaces and then restarted the networking service: <pre><code>$ ip addr flush enp40s0\n$ ip addr flush enp46s0\n$ ip addr flush vmbr0\n$ systemctl restart networking\n</code></pre></p> <p>This let me to be able to reach the webui once again.</p> <p>This happened on my asrock x570 creator, I'm assuming some funky chipset stuff is going on as I'm fairly certain the first nvme slot goes to the cpu and the second goes to chipset.</p>"},{"location":"proxmox/proxmox.html","title":"What is Proxmox?","text":"Proxmox VE <p>Proxmox VE is a complete opensource server virtualization management solution. It offers the ability to manage virtual server (VPS) technology with the Linux OpenVZ and KVM technologies. Proxmox VE offers a web interface accessible after installation on your server which makes management easy, typically needing only a few clicks.</p> <p>Proxmox VE was developed by Proxmox Server Solutions in Austria under the Internet Foundation of Austria and is released under the GNU General Public License. Since it\u2019s an opensource solution it can be customized as per your requirements.</p>"},{"location":"proxmox/proxmox.html#proxmox-ve-features","title":"Proxmox VE Features","text":"Management Flexible Storage Networking Backup &amp; Restore Live Migration &amp; High Availability Cluster"},{"location":"proxmox/proxmox.html#management","title":"Management","text":"<p>Proxmox VE offers a simple web based management interface which is accessible after intstallation on the server. There is no need to install any additional tools or additional management nodes or external database. The management is done through the web interface based on a javascript framework and allows the administrator to control all the features.</p> <p>Features of Management Interface</p> <ul> <li>VNC console, SSL support</li> <li>Based on Javascript Framework</li> <li>Multiple Authentication methods</li> <li>Dynamic updates of Resources</li> <li>Create Virtual Servers &amp; Containers</li> <li>Handles thousands of VMs with Search capability</li> <li>Role based user and permission management.</li> </ul>"},{"location":"proxmox/proxmox.html#flexible-storage","title":"Flexible Storage","text":"<p>Promox offers a flexible storage model. The virtual machine images can either be stored on one or several local storage on shared storages like NFS and SAN.</p> <p>Storing the Virtual Machines on shared storage allows for live migrations of running VMs with no down time.</p> <p>Proxmox VE supported storage model</p> <ul> <li>ZFS</li> <li>NFS Share</li> <li>Ceph RBD</li> <li>ISCI target</li> <li>GlusterFS</li> <li>LVM Group</li> <li>Director ( storage on existing file system ) </li> </ul>"},{"location":"proxmox/proxmox.html#networking","title":"Networking","text":"<p>Proxmox VE uses a bridged networking model. All VMs can share the same bridge as if virtual cables from each guest were plugged into the same switch. The bridge is then connected to the physical network adapters for the host server to which are assigned the ( TCP / IP ) network configuration so that the VMs can communicate to the outside world.</p> <p>Proxmox VE supports VLANS , bonding and network aggregations which allows you to build complex flexible virtual networks for the hosts leveraging the full power of the Linux network stack.</p>"},{"location":"proxmox/proxmox.html#backup-restore","title":"Backup &amp; Restore","text":"<p>Proxmox VE carries an integrated backup tool called as \u201cvzdump\u201d which creates snapshots of virtual guests both for Openvz &amp; KVM. The vzdump tool creates a tarball of the VM or CT data that includes the virtual disks and all the configuration data.</p> <p>Key points</p> <ul> <li>Live backup</li> <li>Backups can be scheduled</li> <li>Graphical User Interface for Backup operations</li> <li>Graphical User Interface for Restore operations</li> <li>command line interface available</li> <li>Monitoring via GUI</li> </ul>"},{"location":"proxmox/proxmox.html#live-migration-high-availability-cluster","title":"Live Migration &amp; High Availability Cluster","text":"<p>Proxmox VE High Availability Cluster enables the definition of high available virtual servers. With the implementation of a cluster you can balance the workload on different hosts, increasing availability of virtual machines.</p> <p>If a virtual machine or container (VM or CT) is configured as HA and the physical host fails, the VM is automatically restarted on one of the remaining Proxmox VE Cluster nodes. In case of hardware maintenance, you can move the virtual machines on another node without any downtime, (or limited downtime).</p> <p>We can move running virtual servers from one physical host to another without downtime.</p>"},{"location":"proxmox/proxmox.html#for-kvm-based-containers","title":"For KVM based containers","text":"<p>The migration of a virtual machine, running from one physical host to another, is done without any interruption. In order to use the live migration, all virtual disks must reside on shared storage, between hosts, as a SAN or NAS.</p>"},{"location":"proxmox/proxmox.html#for-openvz-containers","title":"For OpenVZ containers","text":"<p>It\u2019s possible to achieve containers migration without downtime, even using local storage, so shared storage is not required. (No San or Nas)</p>"},{"location":"react/getting-started.html","title":"Getting Started with Create React App","text":"<p>This project was bootstrapped with Create React App.</p>"},{"location":"react/getting-started.html#available-scripts","title":"Available Scripts","text":"<p>In the project directory, you can run:</p>"},{"location":"react/getting-started.html#npm-start","title":"<code>npm start</code>","text":"<p>Runs the app in the development mode.\\ Open http://localhost:3000 to view it in your browser.</p> <p>The page will reload when you make changes.\\ You may also see any lint errors in the console.</p>"},{"location":"react/getting-started.html#npm-test","title":"<code>npm test</code>","text":"<p>Launches the test runner in the interactive watch mode.\\ See the section about running tests for more information.</p>"},{"location":"react/getting-started.html#npm-run-build","title":"<code>npm run build</code>","text":"<p>Builds the app for production to the <code>build</code> folder.\\ It correctly bundles React in production mode and optimizes the build for the best performance.</p> <p>The build is minified and the filenames include the hashes.\\ Your app is ready to be deployed!</p> <p>See the section about deployment for more information.</p>"},{"location":"react/getting-started.html#npm-run-eject","title":"<code>npm run eject</code>","text":"<p>Note: this is a one-way operation. Once you <code>eject</code>, you can't go back!</p> <p>If you aren't satisfied with the build tool and configuration choices, you can <code>eject</code> at any time. This command will remove the single build dependency from your project.</p> <p>Instead, it will copy all the configuration files and the transitive dependencies (webpack, Babel, ESLint, etc) right into your project so you have full control over them. All of the commands except <code>eject</code> will still work, but they will point to the copied scripts so you can tweak them. At this point you're on your own.</p> <p>You don't have to ever use <code>eject</code>. The curated feature set is suitable for small and middle deployments, and you shouldn't feel obligated to use this feature. However we understand that this tool wouldn't be useful if you couldn't customize it when you are ready for it.</p>"},{"location":"react/getting-started.html#learn-more","title":"Learn More","text":"<p>You can learn more in the Create React App documentation.</p> <p>To learn React, check out the React documentation.</p>"},{"location":"react/getting-started.html#code-splitting","title":"Code Splitting","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/code-splitting</p>"},{"location":"react/getting-started.html#analyzing-the-bundle-size","title":"Analyzing the Bundle Size","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/analyzing-the-bundle-size</p>"},{"location":"react/getting-started.html#making-a-progressive-web-app","title":"Making a Progressive Web App","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/making-a-progressive-web-app</p>"},{"location":"react/getting-started.html#advanced-configuration","title":"Advanced Configuration","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/advanced-configuration</p>"},{"location":"react/getting-started.html#deployment","title":"Deployment","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/deployment</p>"},{"location":"react/getting-started.html#npm-run-build-fails-to-minify","title":"<code>npm run build</code> fails to minify","text":"<p>This section has moved here: https://facebook.github.io/create-react-app/docs/troubleshooting#npm-run-build-fails-to-minify</p>"},{"location":"software/windows-auto-installer.html","title":"Windows Auto Installer","text":"<p>Download Github</p> Use-case <p>This batch script has been made to make it easy to keep track of, and installing software on a freshly re-installed Windows pc.</p> <p>This is done by using the Exe folder to contain all setup.exe and the WAI.bat to run the installation of these setup.exe </p> <p>This will not automated the individual setup.exe </p> <p>It will automated the \"running\" of the next setup.exe when you have finshed a installation. This can be helpfully if you have to do helpdesk tasks while installing software.   </p> How to use it? How to modify it for my use?"},{"location":"software/windows-auto-installer.html#how-to-use-it","title":"How to use it?","text":"<ul> <li> <p>Download the latest release for USB.</p> </li> <li> <p>Move the downloaded zip file to a USB.</p> </li> <li> <p>Copy the zip file to \"target\" machine.</p> </li> </ul> <p>-&gt; You have to manuelly download WordMat and put it into the Exe folder and rename it to \"WordMat.exe\"</p> <p>-&gt; You have to manuelly download Papercut and put it into the Exe folder and rename it to \"Papercut.exe\"</p> <p>-&gt; You have to manuelly download AppWriter and put it into the Exe folder and rename it to \"AppWriter.exe\"</p> <ul> <li>Unzip on the desktop (The only way it will work for now)</li> <li>Open the folder on the desktop </li> <li>Run the WAI.bat</li> </ul>"},{"location":"software/windows-auto-installer.html#easy-way-to-use-it","title":"Easy way to use it","text":"<ul> <li> <p>\"Pre-download\" Wordmat, Papercut and AppWriter</p> </li> <li> <p>Unzip the folder</p> </li> <li> <p>Move Wordmat, Papercut and AppWriter into the Exe folder</p> </li> <li> <p>Zip the folder back up (I recommend zipping it again to avoid running it on the USB drive. This will not work)</p> </li> <li> <p>Move the zip folder to the USB ready for use</p> </li> </ul>"},{"location":"software/windows-auto-installer.html#modify","title":"Modify","text":"<p>This is a \"cell\" or in other words this is the \"variables\" for one pices of software.</p> <pre><code>call \"c:\\Users\\%Username%\\Desktop\\Windows-Auto-Installer-Master-USB\\OfficeSetup.exe\"\nECHO.\necho Please Wait...\ntimeout /t 3 /nobreak &gt; nul\nECHO.\npause\n</code></pre> <p>All you need to change is the <code>call \"c:\\Users\\%Username%\\Desktop\\Windows-Auto-Installer-Master-USB\\Exe\\your-software.exe\"</code></p> <p>You should just put your new exe file into the \"Exe\" folder and change the Exe file name in the end of the string.</p>"},{"location":"unraid/nginx-proxy-manager.html","title":"Setup Nginx Proxy Manager on Unraid","text":"Setup Nginx Proxy Manager on Unraid Configure Nginx Proxy Manager"},{"location":"unraid/nginx-proxy-manager.html#setup-nginx-proxy-manager-on-unraid_1","title":"Setup Nginx Proxy Manager on Unraid","text":"<ul> <li> <p>Go to the CA Apps Tab</p> </li> <li> <p>Search for Nginx-Proxy-Manager-Official</p> </li> <li> <p>Install from mgutt's Repository</p> </li> </ul> <p>You should see a page like this</p> <p></p> <p></p>"},{"location":"unraid/nginx-proxy-manager.html#configure-nginx-proxy-manager","title":"Configure Nginx Proxy Manager","text":""},{"location":"unraid/nginx-proxy-manager.html#login-the-first-time","title":"Login the first time","text":"<ul> <li>Go to the WebGui of Nginx Proxy Manager</li> </ul> <ul> <li>Login the first time with the default login details</li> </ul> <pre><code>Username: admin@example.com\nPassword: changeme\n</code></pre> <ul> <li>Change your login details</li> </ul>"},{"location":"unraid/nginx-proxy-manager.html#getting-cert-and-keys-from-cloudflare","title":"Getting Cert and Keys from Cloudflare","text":"<ul> <li> <p>Login into your cloudflare account </p> </li> <li> <p>Go to the domain you want to use for Nginx Proxy Manager.</p> </li> <li> <p>Then I recommend that you change your SSL/TLS settings to Full(strict)</p> </li> </ul> <p></p> <ul> <li> <p>Next go to Origin Server </p> </li> <li> <p>Create your own Custom Certificates </p> </li> </ul> <p></p> <p>I use rsa 2048 for my own sites and 15 years vaild</p> <ul> <li>Download the Cert and Key </li> </ul> <ul> <li> <p>Copy and paste the code Cloudflare gives you</p> </li> <li> <p>Save it into 2 files Cert.pem and Key.pem (Just use notepad)</p> </li> </ul> <p>Make sure you get the files right</p>"},{"location":"unraid/nginx-proxy-manager.html#setting-nginx-proxy-manager-up-with-cert-and-keys","title":"Setting Nginx Proxy Manager up with Cert and keys","text":"<ul> <li> <p>Go to the WebGui of Nginx Proxy Manager</p> </li> <li> <p>Navigate to SSL Certificates</p> </li> </ul> <p></p> <p>Don't press the add button in the middel of the screen</p> <ul> <li> <p>Use the add SSL Certificate under the admin icon</p> </li> <li> <p>Pick Custom and you should see this </p> </li> </ul> <p></p> <ul> <li>Give it a name, and add the 2 files from Cloudflare</li> </ul> <p>Again make sure you get the files right</p>"},{"location":"unraid/nginx-proxy-manager.html#you-can-now-start-to-add-your-domain-site-and-sub-domains","title":"You can now start to add your domain site and sub-domains","text":""},{"location":"unraid/unraid-cloudflared.html","title":"Cloudflare Tunnel","text":"What is Argo tunnels? Requirements Create App Folder Authorise Cloudflared Create a tunnel Create the config.yaml Install cloudflared in Unraid Setting up your DNS records"},{"location":"unraid/unraid-cloudflared.html#what-is-argo-tunnels","title":"What is Argo tunnels?","text":"<p>Argo Tunnel creates a secure, outbound-only connection between your services and Cloudflare by deploying a lightweight connector in your environment. With this model, your team does not need to go through the hassle of poking holes in your firewall or validating that traffic originated from Cloudflare IPs.</p>"},{"location":"unraid/unraid-cloudflared.html#requirements","title":"Requirements","text":"<p>For this setup, you need to have a domain that is managed by Cloudflare, and can be done on the free plan.</p>"},{"location":"unraid/unraid-cloudflared.html#create-app-folder","title":"Create App Folder","text":"<p>First we need to make sure we have the app folder ready with the correct permissions. Thanks to this tip from our discord user @noodlemctwoodle, you can use the following command in the Unraid terminal to create the folder and set the correct permissions:</p> <pre><code>mkdir -p /mnt/user/appdata/cloudflared/ &amp;&amp; chmod -R 777 /mnt/user/appdata/cloudflared/\n</code></pre>"},{"location":"unraid/unraid-cloudflared.html#authorise-cloudflared","title":"Authorise Cloudflared","text":"<p>In Unraid terminal, run the following command to authorize Cloudflared with the Cloudflare site you want to set up with a tunnel.</p> <pre><code>docker run -it --rm -v /mnt/user/appdata/cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel login\n</code></pre> <p>It will print out a link to Cloudflare. Put this link in your web browser, and select which domain you want to use. Then, the daemon will automatically pull the certificate.</p>"},{"location":"unraid/unraid-cloudflared.html#create-a-tunnel","title":"Create a tunnel","text":"<p>Now we need to create a tunnel. To do this, we will run another command from the Unraid terminal:</p> <pre><code>docker run -it --rm -v /mnt/user/appdata/cloudflared:/home/nonroot/.cloudflared/ cloudflare/cloudflared:latest tunnel create TUNNELNAME\n</code></pre> <p>This will create your tunnel's UUID.json file, which contains a secret used to authenticate your tunnelled connection with Cloudflare. The JSON file is only needed for running the tunnel, but any tunnel modifications require the cert.pem.</p> <p>Make sure you copy your UUID, as this will be used in later steps. It can always be found later by the name of the JSON file.</p>"},{"location":"unraid/unraid-cloudflared.html#create-the-configyaml","title":"Create the config.yaml","text":"<p>Now we need to create a config.yaml to configure the tunnel</p> <pre><code>nano /mnt/user/appdata/cloudflared/config.yml\n</code></pre> <p>Now paste in the following and amend your reverse proxy IP:PORT, tunnel UUID and domain name if applicable</p> <ul> <li> <p>if you have an ssl certificate on your reverse proxy, you need to pass in your domain name that the SSL cert is under</p> </li> <li> <p>if you want to proxy to an http server, use the commended ingress rule</p> </li> <li> <p>if you want to disable ssl verification, add noTLSVerify under originRequest</p> </li> </ul> <pre><code>tunnel: UUID\ncredentials-file: /home/nonroot/.cloudflared/UUID.json\n\n# NOTE: You should only have one ingress tag, so if you uncomment one block comment the others\n\n# forward all traffic to Reverse Proxy w/ SSL\ningress:\n  - service: https://REVERSEPROXYIP:PORT\n    originRequest:\n      originServerName: yourdomain.com\n\n#forward all traffic to Reverse Proxy w/ SSL and no TLS Verify\n#ingress:\n#  - service: https://REVERSEPROXYIP:PORT\n#    originRequest:\n#      noTLSVerify: true\n\n# forward all traffic to reverse proxy over http\n#ingress:\n#  - service: http://REVERSEPROXYIP:PORT\n</code></pre>"},{"location":"unraid/unraid-cloudflared.html#install-cloudflared-in-unraid","title":"Install cloudflared in Unraid","text":"<p>Now, we need to install the app inside the Unraid UI.</p> <ul> <li> <p>Go to the CA Apps Tab</p> </li> <li> <p>Search for cloudflared</p> </li> <li> <p>Install from aeleos' Repository</p> </li> <li> <p>Change the Repository: line to:</p> </li> </ul> <pre><code>cloudflare/cloudflared:latest\n</code></pre> <p>Now we need to change the \"Post Arguments\". To do this we need to enable the \"Advanced View\" in the top right corner.</p> <ul> <li>You should see the below command inside of \"Post Arguments\". Replace UUID the the UUID for your tunnel generated in step 2.</li> </ul> <pre><code>Post arguments: \ntunnel run UUID\n</code></pre> <p>We also need to make a docker net in Unraid so we can later connect Nginx Proxy Manager.</p> <p>Open the terminal for the unraid server and paste the code below, with changes made to the name of the network.</p> <pre><code>docker network create Your-name-for-the-network \n</code></pre> <p>Now pick the network you made from the drop down menu when you are setting up the post arguments.</p> <p>Now you can start your container and if all done correctly with no errors, you should have a running tunnel!</p>"},{"location":"unraid/unraid-cloudflared.html#setting-up-your-dns-records","title":"Setting up your DNS records","text":"<p>The next step will be to edit your domain DNS records.</p> <ul> <li> <p>If you have an A record already, you can remove this as it is now not needed.</p> </li> <li> <p>Replace your A record with a CNAME record, that points to the domain root (@) and for the content, you need to add UUID.cfargotunnel.com (inserting your UUID that was copied earlier).</p> </li> </ul> Type Name Value TTL Status Cname @ UUID.cfargotunnel.com Automatic Cname plex @ Automatic Cname portainer @ Automatic Cname radarr @ Automatic Cname sonarr @ Automatic <p>You should now be able to access all of your apps without needed a port forward!</p>"},{"location":"unraid/unraid.html","title":"What is Unraid?","text":"Network Attached Storage (NAS) server The Unraid Parity-Protected Array User Shares Cache Docker &amp; Virtual Machines (VM)"},{"location":"unraid/unraid.html#network-attached-storage-nas-server","title":"Network Attached Storage (NAS) server","text":"<ul> <li> <p>A Network Attached Storage (NAS) server is a highly accessible data storage server that allows for the storage of information in one location. With a NAS server, you can work on \u2018computer A\u2019 without worrying about a file on \u2018computer B\u2019 in another room that you need to access. Simply download it from your network\u2019s file server and access from any device.</p> </li> <li> <p>Most NAS servers include data redundancy, usually in the form of a RAID array that allows for the failure of one or more hard drives. As the name implies, Unraid does not make use of a RAID array. Instead, it creates data redundancy for each bit in the storage array using a parity drive.</p> </li> <li> <p>An APPLICATION SERVER leverages Docker to host applications ranging from home automation hubs to media servers like Plex. A VM Host uses a hypervisor to provide hardware virtualization and allows hardware to be passed over to the guest OS, letting them utilize hardware that Unraid cannot.</p> </li> </ul>"},{"location":"unraid/unraid.html#the-unraid-parity-protected-array","title":"The Unraid Parity-Protected Array","text":"<p>With Unraid, your data is stored in the drive array. This is a parity-protected drive allowing for the restoration of data in the event of a failure. UnRAID also grants you permission to set up the array using 0 parity drives, 1 parity drive, or 2 parity drives, with 0 providing no protection. </p> <p>Essentially, the number of parity drives you have determines how many drive failures your array can withstand before entirely rebuilding. The blog will be focused on a single parity drive, which creates parity bits across all drives using an XOR calculation.</p> <p>Unraid writes files entirely to a single disk, as opposed to typical RAID levels, which stripe data over all disks in the array. Because it\u2019s just a regular file system and a single disk doesn\u2019t rely on the others, even if you have a catastrophic failure and lose more drives than your array protects, you can still recover data on the drives that haven\u2019t failed. </p> <p>As the data is not striped, you can have disks of various sizes in your array and they will all be used to their full potential. By allowing drives to spin down when not in use, Unraid helps you save energy. Not every drive needs to be spun up even when data is being written. The only requirement is that the parity drive has at least the same capacity as the array\u2019s largest drive.</p>"},{"location":"unraid/unraid.html#user-shares","title":"User Shares","text":"<p>Unraid allows you to arrange your data by creating shares. Each share can contain data stored to any drive in the array, but it will appear as a single folder when viewed from a Windows machine. You can also specify privacy levels for these shares, as well as user access limits.</p>"},{"location":"unraid/unraid.html#cache","title":"Cache","text":"<p>For a cache drive, you\u2019ll usually utilize a SATA or NVME SSD. However, even a regular hard disk will enhance server write times. This is because reading the parity bit, computing the new parity bit, and waiting for the hard drive platter to spin back around to write the bit are all part of the writing process. This is referred to as \u2018read-modify-write\u2019 (RMW for short). </p> <p>Unraid now features a new mode called \u2018reconstruct write,\u2019 in which data is quickly written, all other data disks are read, and parity is determined before being written. There\u2019s no need to wait for the platters to rotate! Also, each drive does not have double I/O. Rather than adjusting parity like before, it is being built from the ground up.</p> <p>Each share can contain data stored to any drive in the array, but it will appear as a single folder when viewed from a Windows machine.</p> <p>A filesystem that spans two 500 GB devices and one 1 TB device, for example, could provide RAID1 for all data, whereas a filesystem that spans a 1 TB device and a single 500 GB device could only provide RAID1 for 500 GB of data.</p>"},{"location":"unraid/unraid.html#docker-virtual-machines-vm","title":"Docker &amp; Virtual Machines (VM)","text":"<p>Docker engine is built into Unraid, and acts to manage images and resources. It also virtualizes the operating system and uses namespaces to isolate. </p> <p>The Docker image self-contains application and its dependencies. </p> <p>A Docker container is a running instance of a docker image. Its Hypervisor virtualizes the hardware, allowing the host operating system to share resources between different virtual machines.</p> <p>A new operating system needs to be installed for each VM you run. Keep in mind, there are hard drive and other resource requirements. VM doesn\u2019t use all its allocated resources all the time and all dependencies need to be reinstalled on each VM, which entails no dependency sharing.</p>"}]}